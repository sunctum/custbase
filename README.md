# Custbase Pipeline

Проект для обработки таможенной базы по импорту/экспорту: от объединения сырых источников до финального датамарта и дашборда.

## Что делает проект

Пайплайн собирает данные из нескольких источников, нормализует компании и страны, классифицирует товарные описания, извлекает бренды и продуктовые атрибуты, после чего формирует финальную витрину для аналитики.

Основной фокус — записи по трубопроводной арматуре (клапаны, краны и смежные категории).

## Структура репозитория

- `pipeline/` — шаги ETL и утилиты.
  - `step0_merger_atlas.py` — ad-hoc объединение нескольких файлов Atlas в один.
  - `step1_preprocess.py` — чтение источников, приведение колонок к единой схеме, нормализация компаний.
  - `step2_tagging.py` — лемматизация и классификация описаний (`одобрено` / `исключено` / `не определено`).
  - `step3_enrichment.py` — обогащение, валидации, флаги аномалий и подозрительных компаний.
  - `step4_brand_extraction.py` — определение бренда по словарю (exact + fuzzy).
  - `step5_attribute_extraction.py` — извлечение технических атрибутов (DN, PN, материал и т.д.).
  - `step6_datamart.py` — формирование финального датамарта и признака релевантности.
  - `utils/` — функции ввода/вывода, логирования и нормализации.
- `data/`
  - `raw/` — входные файлы источников.
  - `st*_*/` — промежуточные этапы обработки.
  - `utilities/` — справочники и вспомогательные словари.
  - `dashboard/` — файл Power BI.
- `main.py` — черновой оркестратор шагов (в текущем виде может требовать доработки под актуальные пути/функции).

## Требования

- Python `>= 3.10.10`
- зависимости из `requirements.txt`

Установка:

```bash
python -m venv .venv
pip install -r requirements.txt
```

## Подготовка данных

Положите исходные файлы в `data/raw/`:

- `EAU.xlsx`
- `atlas.xlsx`
- `rf_world_exp_2025.xlsx`

Также убедитесь, что присутствуют справочники:

- `data/utilities/word_tagger/tagged_words.csv`
- `data/utilities/dict_brand.csv`
- `data/utilities/blacklist_companies.csv`

## Запуск пайплайна

> В скриптах используются импорты вида `from utils...`, поэтому запускать удобно с `PYTHONPATH=./pipeline` из корня репозитория.

Пошаговый запуск:

```bash
PYTHONPATH=./pipeline python pipeline/step1_preprocess.py
PYTHONPATH=./pipeline python pipeline/step2_tagging.py
PYTHONPATH=./pipeline python pipeline/step3_enrichment.py
PYTHONPATH=./pipeline python pipeline/step4_brand_extraction.py
PYTHONPATH=./pipeline python pipeline/step5_attribute_extraction.py
PYTHONPATH=./pipeline python pipeline/step6_datamart.py
```

Ожидаемый финальный файл: `data/st6_datamart/st6.xlsx`.

## Логика обработки (кратко)

1. **Препроцессинг**
   - унификация схемы колонок между источниками;
   - нормализация названий импортеров/экспортеров и ОПФ.
2. **Тегирование текстов**
   - нормализация смешанной кириллицы/латиницы;
   - лемматизация;
   - учет отрицаний (например, «не является клапаном»).
3. **Обогащение и контроль качества**
   - унификация стран;
   - перерасчет цены/веса для дублей деклараций;
   - флаги аномалий цены за кг;
   - маркировка подозрительных/blacklist компаний.
4. **Бренд и атрибуты**
   - извлечение бренда по словарю и fuzzy-матчингу;
   - извлечение DN/PN, материала, типа изделия, уплотнения.
5. **Датамарт**
   - вычисление `is_relevant` и причин нерелевантности;
   - очистка служебных полей.

## Полезные замечания

- `pipeline/step0_merger_atlas.py` содержит локальные Windows-пути и используется как служебный ad-hoc скрипт.
- При первом запуске `step2_tagging.py` выполняется загрузка NLTK stopwords.
- В `dev_notes.md` есть журнал изменений и TODO.

## Развитие проекта

Ближайшие улучшения:

- дооформление по PEP8;
- стабилизация обработки стран из нового источника;
- улучшение поиска DN/PN;
- дополнения в step5 (пополнение списка атрибутов, настройка логики под них)
- настройка оркестратора (`main.py`) под актуальный пайплайн.